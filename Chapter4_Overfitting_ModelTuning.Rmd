---
title: 'Chapter 4: Overfitting and Model Tuning'
author: "Niko Steinhoff"
output: 
    html_document:
        toc: true
---


___
# Data Splitting
```{r 'Loading two-class data'}
library(AppliedPredictiveModeling)
data(twoClassData)

str(predictors)
str(classes)

## using scikit-learn notation
# Capital X is the matrix of predictors
X <- predictors
# Lowercase y is the vector of classification labels
y <- classes

```


## Simple training/test split
```{r 'Simple Partitioning'}
library(caret)

# Set random seed so results become reproducible
set.seed(1)

# Stratified data partitioning with 80% of samples in the training set. 
# Returns a matrix of row numbers. In this case the matrix has only one column,
# because we only sample once. Resampling will add additional columns.
trainingRows <- createDataPartition(y,
                                    p = 0.8,
                                    list = FALSE)
# Convert to vector (not necessary)
trainingRows <- trainingRows[, 1]

head(trainingRows)

# Subsetting by training row indices and converting predictors to matrix.
X_train <- as.matrix(X[trainingRows, ])
y_train <- y[trainingRows]

X_test <- as.matrix(X[-trainingRows, ])
y_test <- y[-trainingRows]

# Check the ratio between training and test sets
dim(X_train)[1]/(dim(X_test)[1] + dim(X_train)[1])
```

## Resampling methods
### Repeated training/test splits
```{r 'Repeated training/test splits'}
# Set random seed so results become reproducible
set.seed(1)

# Creating 3 different splits
repeatedSplits <- createDataPartition(y,
                                      p = 0.8,
                                      times = 3)

str(repeatedSplits)
```

### Bootstrap
```{r 'Bootstrap'}
# Set random seed so results become reproducible
set.seed(1)

# Creating 10 different bootstrapped samples
bootstrap <- createResample(y, times = 10)
str(bootstrap)
# Frequencies of rows shows that some samples have been selected multiple times
table(bootstrap[[1]])

# Checking which rows were not selected. These would be used for testing.
notInBootstrap <- lapply(bootstrap, function(x){
    setdiff(1:length(x), x)
})
str(notInBootstrap)
```

### Folds
```{r 'Folds'}
# Set random seed so results become reproducible
set.seed(1)

# Creating k-fold split
# Returns the training rows for each fold
trainingFolds <- createFolds(y, k = 10, returnTrain = TRUE)
str(trainingFolds)

# Training/test split for the first fold
X_train_fold1 <- X[trainingFolds[[1]], ]
y_train_fold1 <- y[trainingFolds[[1]]]

X_test_fold1 <- X[-trainingFolds[[1]], ]
y_test_fold1 <- y[-trainingFolds[[1]]]


# Creating multiple independent k-fold splits
multifolds <- createMultiFolds(y, k = 10, times = 5)
str(multifolds[1:25])
```


___
# Basic model building
```{r 'KNN'}
# Fitting a k-nearest neighbors model
knnFit <- knn3(x = X_train, y = y_train, k = 5)
knnFit

# Predicting test data
knnPrediction <- predict(knnFit, newdata = X_test, type = "class")
head(knnPrediction)

# Check performance
confusionMatrix(data = knnPrediction, reference = y_test)
```


___
# Determining tuning parameters
The train function of the the 'caret' package has built in capabilities for
different resampling methods and performance measures. 

## Tuning an SVM Model

Preparing the data
```{r 'Tuning an SVM Model: Data preparation'}
# Data about credit scoring
library(caret)
data(GermanCredit)

# Removing near-zero variance predictors
GermanCredit <- GermanCredit[ , -nearZeroVar(GermanCredit)]

# We have to remove some dummy variables to avoid linear dependence
str(GermanCredit)
GermanCredit <- subset(GermanCredit, 
                       select = -c(CheckingAccountStatus.lt.0, 
                                   SavingsAccountBonds.lt.100,
                                   EmploymentDuration.lt.1,
                                   EmploymentDuration.Unemployed,
                                   Personal.Male.Married.Widowed,
                                   Property.Unknown,
                                   Housing.ForFree)
                       )

# Split the data into training (80%) and test (20%) sets
set.seed(100)
inTrain <- createDataPartition(GermanCredit$Class,
                               p = 0.8,
                               list = FALSE)
GermanCreditTrain <- GermanCredit[inTrain, ]
GermanCreditTest <- GermanCredit[-inTrain, ]
```

Tuning the model
```{r 'Tuning an SVM: Tuning the model'}
library(kernlab)
set.seed(231)
# Estimating tuning parameter Sigma
sigDist <- sigest(Class ~ .,
                  data = GermanCreditTrain,
                  frac = 1)

svmTuneGrid <- data.frame(sigma = as.vector(sigDist)[1],
                          C = 2^(-2:7))

# Fitting the model
set.seed(1056)
svmFit <- train(Class ~ .,
                data = GermanCreditTrain,
                method = "svmRadial",
                preProc = c("center", "scale"),
                tuneGrid = svmTuneGrid,
                trControl = trainControl(method = "repeatedcv",
                                         repeats = 5,
                                         classProbs = TRUE))

svmFit
```

Plotting the performance profile
```{r 'SVM performance profile'}
# Line plot of average performance
plot(svmFit, scales = list(x = list(log = 2)))
```

Predicting the test set
```{r 'Predict test set'}
predictedClasses <- predict(svmFit, GermanCreditTest)
str(predictedClasses)

# Check classification performance
confusionMatrix(data = predictedClasses, reference = GermanCreditTest$Class)

# Class probabilities
predictedProbs <- predict(svmFit,
                          newdata = GermanCreditTest,
                          type = "prob")
head(predictedProbs)
```


___
# Between-Model Comparison
